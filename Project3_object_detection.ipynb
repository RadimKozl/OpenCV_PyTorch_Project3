{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9756403,"sourceType":"datasetVersion","datasetId":5973896},{"sourceId":10169901,"sourceType":"datasetVersion","datasetId":6280772}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <font style=\"color:blue\">Project 3: Object Detection</font>\n\n\n\n\n\n#### Maximum Points: 100\n\n\n\n|Sr. no.|Section|Points|\n|:-------|:-------|:------:|\n|1|Plot Ground Truth Bounding Boxes|20|\n|2|Training|25|\n|3|Inference|15|\n|4|COCO Detection Evaluation|25|\n|5|COCO Detection Evaluation|15|","metadata":{}},{"cell_type":"markdown","source":"# <font style=\"color:purple\">Download the config file and trainer package</font> ","metadata":{}},{"cell_type":"code","source":"!wget \"https://raw.githubusercontent.com/RadimKozl/OpenCV_PyTorch_Project3/refs/heads/main/trainer.zip\" -O ./trainer.zip\n!ls /kaggle/working/\n!unzip /kaggle/working/trainer.zip\n!rm /kaggle/working/trainer.zip","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wget \"https://raw.githubusercontent.com/RadimKozl/OpenCV_PyTorch_Project3/refs/heads/main/config.yaml\" -O ./config.yaml","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <font style=\"color:purple\">Install external libraries</font> ","metadata":{}},{"cell_type":"code","source":"!pip install psutil \n!pip install h5py\n!pip install pycocotools","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <font style=\"color:purple\">Imports libraries</font> ","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport json\nimport h5py\nimport csv\nimport yaml\nimport shutil\nimport pandas as pd\nimport numpy as np\nimport random\nimport time\nimport psutil\nimport matplotlib.patches as patches\nimport matplotlib.pyplot as plt\nimport IPython\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom concurrent.futures import ThreadPoolExecutor\n\nfrom operator import itemgetter\nimport multiprocessing as mp\nmp.set_start_method('spawn', force=True)\n\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms import functional as F\n\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import MultiStepLR\nfrom torch.nn.parallel import DataParallel\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import faster_rcnn\nimport torchvision.models as models\n\ntorch.multiprocessing.set_start_method('spawn', force=True)\n\nimport torchvision.models as models\n\nfrom torchvision.ops import MultiScaleRoIAlign\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torchvision.models.detection import FasterRCNN","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from trainer import RCNNTrainer, hooks, configuration\nfrom trainer.utils import patch_configs\nfrom trainer.utils import setup_system\n\nfrom trainer.metrics import APEstimator\nfrom trainer.matplotlib_visualizer import MatplotlibVisualizer, DataEmbedingVisualizer, PRVisualizer\nfrom trainer.matplotlib_visualizer import TensorBoardVisualizer, ModelVisualizer, WeightsHistogramVisualizer\nfrom trainer.tensorboard_visualizer import set_writer\n\nfrom trainer.datasets import HDF5Dataset\nfrom trainer.utils import collate_fn\nfrom trainer.configuration import load_config_from_yaml\n\nfrom trainer import RCNNTrainer, hooks, configuration\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:orange\">Function for check memory management</font> ","metadata":{}},{"cell_type":"code","source":"def memory_management():\n    \n    cpu_percent = psutil.cpu_percent(interval=1)\n    print(f\"CPU Usage: {cpu_percent}%\")\n    \n    memory_usage = psutil.virtual_memory()\n    print(f\"Memory Usage: {memory_usage.percent}%\")\n    \n    disk_usage = psutil.disk_usage('/')\n    print(f\"Disk Usage: {disk_usage.percent}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <font style=\"color:purple\">Start TensorBoard by Ngrog tunnel</font> ","metadata":{}},{"cell_type":"code","source":"!tensorboard --version","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n!tar xf ./ngrok-v3-stable-linux-amd64.tgz -C /usr/local/bin","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"pool = mp.Pool(processes = 10)\nresults_of_processes = [pool.apply_async(os.system, args=(cmd, ), callback = None )\n                        for cmd in [\n                        f\"tensorboard --logdir /kaggle/working/faster_rcnn --load_fast=false --host 0.0.0.0 --port 6006 &\",\n                        \"/usr/local/bin/ngrok http 6006 &\"\n                        ]]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <font style=\"color:purple\">Download the Dataset</font> \n\n\n\n**[Download the Vehicle registration plate](https://www.dropbox.com/s/k81ljpmzy3fgtx9/Dataset.zip?dl=1)**\n\n\n\n\n\nDownload the Vehicle Registration Plate dataset from [here](https://www.dropbox.com/s/k81ljpmzy3fgtx9/Dataset.zip?dl=1) and unzip it. \n\n\n\nWe will have the following directory structure:\n\n\n\n```\n\nDataset\n\n├── train\n\n│   └── Vehicle registration plate\n\n│       └── Label\n\n└── validation\n\n    └── Vehicle registration plate\n\n        └── Label\n\n```\n\n\n\nUnzipping the file will give you a directory `Dataset`. This directory has two folder `train` and `validation`. Each train and validation folder has `Vehicle registration plate`  folder with `.jpg` images and a folder `Labels`.  `Labels` folder has bounding box data for the images.\n\n\n\n\n\nFor example,\n\nFor image: `Dataset/train/Vehicle registration plate/bf4689922cdfd532.jpg`\n\nLabel file is  `Dataset/train/Vehicle registration plate/Label/bf4689922cdfd532.txt`\n\n\n\nThere are one or more lines in each `.txt` file. Each line represents one bounding box.\n\nFor example,\n\n```\n\nVehicle registration plate 385.28 445.15 618.24 514.225\n\nVehicle registration plate 839.68 266.066462 874.24 289.091462\n\n```\n\n\n\nWe have a single class detection (`Vehicle registration plate detection`) problem. So bounding box details start from the fourth column in each row.\n\n\n\nRepresentation is in `xmin`, `ymin`, `xmax`, and `ymax` format.\n\n\n\n**It has `5308` training and `386` validation dataset.**\n\n\n\nData is downloaded from [Open Images Dataset](https://storage.googleapis.com/openimages/web/index.html)","metadata":{}},{"cell_type":"markdown","source":"#  <font style=\"color:green\">1. Plot Ground Truth Bounding Boxes [20 Points]</font> \n\n\n\n**You have to show three images from validation data with the bounding boxes.**\n\n\n\nThe plotted images should be similar to the following:\n\n\n\n<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-g1.png'>\n\n\n\n\n\n\n\n<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-g2.png'>\n\n\n\n\n\n\n\n<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-g3.png'>\n\n\n","metadata":{}},{"cell_type":"markdown","source":"##  <font style=\"color:blue\">1.1. Class for create HDF5 file of datasets</font>\n\nWe create Json structure of `train`, `valid` and `test` datasets, for creating PyTorch Dataloader a standard module of trainer. We add test samples as [OpenCV_evalution_ALPR_dataset](https://www.kaggle.com/datasets/radimkzl/opencv-evalution-alpr-dataset).","metadata":{}},{"cell_type":"code","source":"class TransformDataset:\n    def __init__(self, name_hdf5_file='datasets.hdf5', hdf5_image=False):\n        self.name_hdf5_file = name_hdf5_file\n        self.path_test_images = os.path.join('/kaggle', 'input', 'opencv-evalution-alpr-dataset', 'cars_ALPR_test', 'images')\n        self.path_test_annotations = os.path.join('/kaggle', 'input', 'opencv-evalution-alpr-dataset', 'cars_ALPR_test','Annotations')\n        self.path_train_images = os.path.join('/kaggle', 'input', 'vehicle-registration-plate', 'Dataset', 'train', 'Vehicle registration plate')\n        self.path_valid_images = os.path.join('/kaggle', 'input', 'vehicle-registration-plate', 'Dataset', 'validation', 'Vehicle registration plate')\n        self.hdf5_file = None\n        self.hdf5_image = hdf5_image\n        \n\n    def __select_files(self, root_path, extensions=('.jpg', '.png')):\n        \"\"\"\n        Select image files from the given directory with specified extensions.\n\n        Args:\n            root_path (str): Path to the directory containing images.\n            extensions (tuple): Valid file extensions.\n\n        Returns:\n            list: List of file paths matching the extensions.\n        \"\"\"\n        if not os.path.exists(root_path):\n            print(f\"Warning: Path {root_path} does not exist!\")\n            return []\n\n        return [os.path.join(root_path, filename) for filename in os.listdir(root_path) if filename.endswith(extensions)]\n\n    def __get_image_shape(self, image_path):\n        \"\"\"\n        Get the shape (width, height, channels) of an image.\n\n        Args:\n            image_path (str): Path to the image file.\n\n        Returns:\n            list: [width, height, channels]\n        \"\"\"\n        channel = None\n        img = Image.open(image_path)\n        w, h = img.size\n        c = img.mode\n        if c == 'RGB':\n            channel = 3\n        return [w, h, channel]\n\n    def __select_data(self, file_path):\n        \"\"\"\n        Parse annotation data from a file.\n\n        Args:\n            file_path (str): Path to the annotation file.\n\n        Returns:\n            tuple: Tuple of data (label_id, box_coordinates)\n        \"\"\"\n        box_coordinates = []\n        labels_id = []\n        list_class_name = ['__background__', 'Vehicle registration plate']\n\n        if not os.path.isfile(file_path):\n            print(f\"Warning: Annotation file {file_path} not found!\")\n            return []\n\n        with open(file_path, 'r') as input_file:\n            lines = input_file.readlines()\n\n        for line in lines:\n            parts = line.strip().split()\n            if len(parts) < 5:\n                print(f\"Warning: Invalid annotation format in file {file_path}: {line.strip()}\")\n                continue\n\n            label = parts[0] + ' ' + parts[1] + ' ' + parts[2]\n            numbers = [round(float(num)) for num in parts[3:]]\n\n            labels_id.append(list_class_name.index(label))\n            box_coordinates.append(numbers)\n\n        return (labels_id, box_coordinates)\n\n    def process_files(self, file_paths, dataset_type):\n        \"\"\"\n        Process image and annotation files and add them to the HDF5 dataset.\n\n        Args:\n            file_paths (list): List of image file paths.\n            dataset_type (str): Dataset type ('train', 'valid', 'test').\n        \"\"\"\n        for path in tqdm(file_paths, desc=f\"Processing {dataset_type} files\"):\n            try:\n                # Get image dimensions\n                img_dim = self.__get_image_shape(path)\n                name_dir, name_input = os.path.split(path)\n                id_name, file_ext = os.path.splitext(name_input)\n\n                # Determine label file path\n                if dataset_type == 'test':\n                    # Test dataset annotations are in a separate directory\n                    path_label = os.path.join(self.path_test_annotations, f\"{id_name}.txt\")\n                else:\n                    annotations_dir = os.path.join(name_dir, 'Label')\n                    path_label = os.path.join(annotations_dir, f\"{id_name}.txt\")\n\n                # Read and process annotation data\n                sub_data = self.__select_data(path_label)\n                if not sub_data:\n                    print(f\"Warning: No annotations found for {path_label}. Skipping.\")\n                    continue\n\n                # Prepare data dictionary\n                data = {\n                    \"name_id\": str(id_name),\n                    \"name\": str(name_input),\n                    \"path\": str(path),\n                    \"type\": file_ext.lstrip('.'),\n                    \"dimension\": img_dim,\n                    \"labels\": sub_data[0],\n                    \"boxes\": sub_data[1]\n                }\n\n                try:\n                    # Serialize data and write to HDF5\n                    # set dataset group\n                    dataset_group = self.hdf5_file[dataset_type]\n                    # Create a subgroup for the image\n                    image_group = dataset_group.create_group(data['name_id'])\n                    if self.hdf5_image:\n                        # Add image path, labels, and boxes to the image group\n                        img = Image.open(path)\n                        img_array = np.array(img)\n                        image_group.create_dataset('image', data=img_array)\n                    \n                        image_group.attrs['name'] = np.array(data['name'], dtype='S100')\n                        image_group.attrs['type'] = np.array(data['type'], dtype='S3')\n                        image_group.attrs['dimension'] = np.array(data['dimension'], dtype='float64')\n                        image_group.attrs['labels'] = data['labels']\n                        image_group.attrs['boxes'] = data['boxes']\n                        image_group.attrs['link'] = np.array(path, dtype='S200')\n                    else:\n                        image_group.create_dataset('image_link', data=[path])\n                        image_group.attrs['name'] = np.array(data['name'], dtype='S100')\n                        image_group.attrs['type'] = np.array(data['type'], dtype='S3')\n                        image_group.attrs['dimension'] = np.array(data['dimension'], dtype='float64')\n                        image_group.attrs['labels'] = data['labels']\n                        image_group.attrs['boxes'] = data['boxes']\n                        \n                except Exception as e:\n                    print(f\"Error processing: {e}\")\n            \n\n            except Exception as e:\n                print(f\"Error processing file {path}: {e}\")\n\n    def create_hdf5_data(self):\n        \"\"\"\n        Create HDF5 dataset file with train, valid, and test groups.\n        \"\"\"\n        # Select files for each dataset type\n        list_train_files = self.__select_files(self.path_train_images, ('.jpg', '.png'))\n        list_valid_files = self.__select_files(self.path_valid_images, ('.jpg', '.png'))\n        list_test_files = self.__select_files(self.path_test_images, ('.jpg', '.png'))\n\n        # Validate file lists\n        if not list_train_files:\n            print(\"No training files found!\")\n        if not list_valid_files:\n            print(\"No validation files found!\")\n        if not list_test_files:\n            print(\"No test files found!\")\n\n        with h5py.File(os.path.join('/kaggle', 'working', self.name_hdf5_file), 'w') as self.hdf5_file:\n            # Create dataset groups\n            self.hdf5_file.create_group('train')\n            self.hdf5_file.create_group('valid')\n            self.hdf5_file.create_group('test')\n\n            # Add metadata attributes\n            self.hdf5_file.attrs['class_number'] = 2\n            self.hdf5_file.attrs['names_class'] = ['__background__', 'Vehicle registration plate']\n\n            # Process and add files\n            self.process_files(list_train_files, 'train')\n            self.process_files(list_valid_files, 'valid')\n            self.process_files(list_test_files, 'test')\n\n    def close(self):\n        if hasattr(self, 'name_hdf5_file') and self.name_hdf5_file:\n            self.hdf5_file.close()\n        else:\n            try:\n                f = h5py.File(os.path.join('/kaggle', 'working', self.name_hdf5_file), 'w')\n                f.close()\n                print(\"File is closed...\")\n            except:\n                  print(\"File is still locked, we close file...\")  \n            finally:\n                self.hdf5_file.close()\n\n        print(\"HDF5 dataset created successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  <font style=\"color:blue\">1.2. Create HDF5 file of datasets</font>","metadata":{}},{"cell_type":"code","source":"%%time\ntransform_dataset = TransformDataset(name_hdf5_file='datasets.hdf5', hdf5_image=False)\ntransform_dataset.create_hdf5_data()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform_dataset.close()\ndel transform_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gc.collect()\nmemory_management()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  <font style=\"color:blue\">1.3. Create Dataloaders for read data</font>","metadata":{}},{"cell_type":"markdown","source":"###  <font style=\"color:orange\">Plot function of samples of data</font>","metadata":{}},{"cell_type":"code","source":"def plot_images(loader, rows=2, columns=3):\n    \"\"\"\n    Plot few images with their bounding boxes.\n\n    Args:\n        loader (DataLoader): DataLoader containing the dataset.\n        rows (int): Number of rows in the plot.\n        columns (int): Number of columns in the plot.\n    \"\"\"\n    plt.rcParams[\"figure.figsize\"] = (15, 9)\n    plt.figure()\n\n    for images, targets in loader:\n        for i in range(min(len(targets), rows * columns)):\n            plt.subplot(rows, columns, i + 1)\n            img = F.to_pil_image(images[i])\n            plt.imshow(img)\n\n            # Get bounding boxes and labels\n            boxes = targets[i]['boxes'].numpy()\n            labels = targets[i]['labels'].numpy()\n\n            # Create an Axes instance\n            ax = plt.gca()\n\n            # Plot each bounding box\n            for box, label in zip(boxes, labels):\n                xmin, ymin, xmax, ymax = box\n                width = xmax - xmin\n                height = ymax - ymin\n                rect = patches.Rectangle((xmin, ymin), width, height, linewidth=2, edgecolor='r', facecolor='none')\n                ax.add_patch(rect)\n                plt.text(xmin, ymin, f'Label: {label}', color='red', fontsize=12, backgroundcolor='none')\n\n            plt.axis('off')\n\n        plt.show()\n        break\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  <font style=\"color:orange\">1.3.1 Create train dataset</font>","metadata":{}},{"cell_type":"code","source":"path_hdf5_file = os.path.join('/kaggle','working','datasets.hdf5')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nhdf5_train_dataset = HDF5Dataset(path_hdf5_file, 'train')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n# Calculate mean and std of train dataset\nmean_train, std_train = hdf5_train_dataset.calculate_mean_std_manual()\nprint(f\"Calculated Mean: {mean_train}, Std: {std_train} of train dataset.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# show samples\ntrain_dataloader = torch.utils.data.DataLoader(\n    hdf5_train_dataset, \n    batch_size=6, \n    shuffle=True,\n    collate_fn=collate_fn,\n    num_workers=0,\n    pin_memory=True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_images(train_dataloader, rows=2, columns=3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del train_dataloader","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  <font style=\"color:orange\">1.3.2 Create valid dataset</font>","metadata":{}},{"cell_type":"code","source":"%%time\nhdf5_valid_dataset = HDF5Dataset(path_hdf5_file, 'valid')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n# Calculate mean and std of train dataset\nmean_valid, std_valid = hdf5_valid_dataset.calculate_mean_std_manual()\nprint(f\"Calculated Mean: {mean_valid}, Std: {std_valid} of valid dataset.\")\ndel mean_valid, std_valid","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# show samples\nvalid_dataloader = torch.utils.data.DataLoader(\n    hdf5_valid_dataset, \n    batch_size=6, \n    shuffle=True,\n    collate_fn=collate_fn,\n    num_workers=0,\n    pin_memory=True\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_images(valid_dataloader, rows=2, columns=3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del valid_dataloader, hdf5_valid_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  <font style=\"color:orange\">1.3.3 Create test dataset</font>","metadata":{}},{"cell_type":"code","source":"%%time\nhdf5_test_dataset = HDF5Dataset(path_hdf5_file, 'test')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n# Calculate mean and std of train dataset\nmean_test, std_test = hdf5_test_dataset.calculate_mean_std_manual()\nprint(f\"Calculated Mean: {mean_test}, Std: {std_test} of train dataset.\")\ndel mean_test, std_test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# show samples\ntest_dataloader = torch.utils.data.DataLoader(\n    hdf5_test_dataset, \n    batch_size=6, \n    shuffle=True,\n    collate_fn=collate_fn,\n    num_workers=0,\n    pin_memory=True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_images(test_dataloader, rows=2, columns=3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del test_dataloader, hdf5_test_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  <font style=\"color:orange\">1.3.4 Show class names</font>","metadata":{}},{"cell_type":"code","source":"list_class_names = hdf5_train_dataset.names_class\n\n# Access metadata\nprint(f\"Class Number: {hdf5_train_dataset.class_number}\")\nprint(f\"Class Names: {list_class_names}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gc.collect()\nmemory_management()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  <font style=\"color:green\">2. Training [25 Points]</font> \n\n\n\n- **Write your training code in this section.**\n\n\n\n- **You also have to share ([shared logs example](https://tensorboard.dev/experiment/JRtnsKbwTaq1ow6nPLPGeg)) the loss plot of your training using tensorboard.dev.** \n\n\n\nHow to share TensorBoard logs using tensorboard.dev find [here](https://courses.opencv.org/courses/course-v1:OpenCV+OpenCV-106+2019_T1/courseware/b1c43ffe765246658e537109e188addb/d62572ec8bd344db9aeae81235ede618/4?activate_block_id=block-v1%3AOpenCV%2BOpenCV-106%2B2019_T1%2Btype%40vertical%2Bblock%40398b46ddcd5c465fa52cb4d572ba3229).","metadata":{}},{"cell_type":"markdown","source":"##  <font style=\"color:blue\">2.1. Create Model</font>","metadata":{}},{"cell_type":"markdown","source":"###  <font style=\"color:orange\">2.1.1 Simple model</font>","metadata":{}},{"cell_type":"code","source":"def faster_rcnn_pretrained_model(num_classes):\n    # load an instance detection model pre-trained on COCO\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='DEFAULT')\n\n    # get the number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    \n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  <font style=\"color:orange\">2.1.2 Model update / proposed model (Fine-tuning)</font>","metadata":{}},{"cell_type":"code","source":"def faster_rcnn_pretrained_model_alexnet(num_classes):\n    \n    # Get the backbone of any pretrained network, we'll use AlexNet\n    alexnet = models.alexnet(pretrained=True)\n    new_backbone = alexnet.features\n    new_backbone.out_channels = 256\n    \n    # Configure the anchors. We shall have 12 different anchors.\n    new_anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256),),\n                                          aspect_ratios=((0.5, 1.0, 2.0),))\n    \n    # Configure the output size of ROI-Pooling layer.\n    # We shall end up with (num_boxes, num_features, 4, 4) after the ROIPooling layer\n    new_roi_pooler = MultiScaleRoIAlign(featmap_names=['0'], output_size=4, sampling_ratio=1)\n    \n    \n    # let's use dummy variables for mean, std, min_size and max_size\n    min_size = 300\n    max_size = 1050\n    mean = mean_train\n    std = std_train\n   \n    # Instantiate the Faster-rcnn model with the variables declared above.\n    frcnn_model = FasterRCNN(backbone=new_backbone,\n                          num_classes=num_classes,\n                          min_size=min_size,\n                          max_size=max_size,\n                          image_mean=mean,\n                          image_std=std,\n                          rpn_anchor_generator=new_anchor_generator,\n                          box_roi_pool=new_roi_pooler)\n\n    return frcnn_model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  <font style=\"color:blue\">2.2. Config file</font>","metadata":{}},{"cell_type":"markdown","source":"###  <font style=\"color:orange\">2.2.1 Update config file</font>","metadata":{}},{"cell_type":"code","source":"config_ex_path = os.path.join('/kaggle','working','config.yaml') # path of yaml file","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load yaml file\nwith open(config_ex_path, 'r') as f:\n    config = yaml.safe_load(f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# set system config values\nconfig['system']['seed'] = 42\nconfig['system']['cudnn_benchmark_enabled'] = False\nconfig['system']['cudnn_deterministic'] = True\n\n# set dataset config values\nconfig['dataset']['root_dir'] = os.path.join('/','kaggle','working')\nconfig['dataset']['hdf5_file'] = \"datasets.hdf5\"\n\n# set dataloader config values\nconfig['dataloader']['batch_size'] = 250\nconfig['dataloader']['num_workers'] = 0\nconfig['dataloader']['data_augmentation'] = False\n\n# set optimizer config values\nconfig['optimizer']['learning_rate'] = 0.001\nconfig['optimizer']['momentum'] = 0.9\nconfig['optimizer']['weight_decay'] = 0.0001\nconfig['optimizer']['lr_step_milestones'] = [30, 40]\nconfig['optimizer']['lr_gamma'] = 0.1\n\n# set trainer config values\nconfig['trainer']['model_dir'] = os.path.join('/kaggle','working','modelscheckpoints')\nconfig['trainer']['model_save_best'] = True\nconfig['trainer']['model_saving_frequency'] = 1\nconfig['trainer']['device'] = \"cuda\"\nconfig['trainer']['epoch_num'] = 30\nconfig['trainer']['progress_bar'] = False\nconfig['trainer']['normalization'] = False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(config_ex_path, 'w') as f:\n    yaml.dump(config, f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  <font style=\"color:orange\">2.2.2 Load Config data from file</font>","metadata":{}},{"cell_type":"code","source":"system_config, dataset_config, dataloader_config, optimizer_config, trainer_config = load_config_from_yaml(config_ex_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('system_config\\n', system_config)\nprint('\\ndataset_config\\n', dataset_config)\nprint('\\ndataloader_config\\n', dataloader_config)\nprint('\\noptimizer_config\\n', optimizer_config)\nprint('\\ntrainer_config\\n', trainer_config)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  <font style=\"color:blue\">2.2. Create Experiment Class</font>","metadata":{}},{"cell_type":"code","source":"class Experiment:\n    def __init__(\n        self,\n        system_config: configuration.SystemConfig = system_config,\n        dataset_config: configuration.DatasetConfig = dataset_config,  \n        dataloader_config: configuration.DataloaderConfig = dataloader_config,\n        optimizer_config: configuration.OptimizerConfig = optimizer_config,\n    ):\n        self.system_config = system_config\n        setup_system(system_config)\n        self.model_name = 'faster_rcnn_pretrained'\n        self.tb_writer = set_writer(os.path.join('faster_rcnn',self.model_name))\n        \n        # fruit detection data has 1-classes. Anything other than these three classes is called background\n        self.classes = list_class_names\n        \n        # written custom dataset class of our dataset\n        self.dataset_train = HDF5Dataset(\n            hdf5_file=path_hdf5_file, \n            dataset_type='train',\n            transform=None,\n            train=True,\n            width_image=1024\n        )\n          \n        self.loader_train = torch.utils.data.DataLoader(\n            dataset=self.dataset_train,\n            batch_size=dataloader_config.batch_size,\n            shuffle=True,\n            collate_fn=collate_fn,\n            num_workers=dataloader_config.num_workers,\n            pin_memory=True\n        )\n        \n\n        self.dataset_valid = HDF5Dataset(\n            hdf5_file=path_hdf5_file, \n            dataset_type='valid',\n            transform=None,\n            train=False,\n        )\n        \n        \n        self.loader_valid = torch.utils.data.DataLoader(\n            dataset=self.dataset_valid,\n            batch_size=dataloader_config.batch_size,\n            shuffle=False,\n            collate_fn=collate_fn,\n            num_workers=dataloader_config.num_workers,\n            pin_memory=True\n        )\n\n        self.dataset_test = HDF5Dataset(\n            hdf5_file=path_hdf5_file, \n            dataset_type='test',\n            transform=None,\n            train=False,\n        )\n        \n        \n        self.loader_test = torch.utils.data.DataLoader(\n            dataset=self.dataset_test,\n            batch_size=dataloader_config.batch_size,\n            shuffle=False,\n            collate_fn=collate_fn,\n            num_workers=dataloader_config.num_workers,\n            pin_memory=True\n        )\n\n        if self.tb_writer is not None:\n            add_data_embedings = DataEmbedingVisualizer(dataset=self.dataset_valid, writer=self.tb_writer, number_samples=50)\n            add_data_embedings.update_charts()\n\n        \n        # get faster rcnn model pretrained on coco\n        #self.model = faster_rcnn_pretrained_model_alexnet(len(self.classes))\n        self.model = faster_rcnn_pretrained_model(len(self.classes))\n        \n        self.metric_fn = APEstimator(classes=self.classes)\n        \n        params = [p for p in self.model.parameters() if p.requires_grad]\n        self.optimizer = optim.SGD(\n            params,\n            lr=optimizer_config.learning_rate,\n            weight_decay=optimizer_config.weight_decay,\n            momentum=optimizer_config.momentum\n        )\n        self.lr_scheduler = MultiStepLR(\n            self.optimizer, milestones=optimizer_config.lr_step_milestones, gamma=optimizer_config.lr_gamma\n        )\n        self.visualizer = TensorBoardVisualizer(writer=self.tb_writer)\n\n    def run(self, trainer_config: trainer_config) -> dict:  \n        setup_system(self.system_config)\n        device = torch.device(trainer_config.device)\n        self.model = self.model.to(device)\n\n        # add network graph with inputs info\n        graph_loader = torch.utils.data.DataLoader(\n            dataset=self.dataset_train,\n            batch_size=1,\n            shuffle=True,\n            collate_fn=collate_fn,\n            num_workers=0,\n            pin_memory=True\n        )\n        \n        inputs, targets = next(iter(graph_loader))\n        inputs = inputs.to(trainer_config.device)\n        \n        add_network_graph_tensorboard = ModelVisualizer(self.model, images, self.tb_writer)\n        add_network_graph_tensorboard.update_charts()\n\n        model_trainer = RCNNTrainer(\n            model=self.model,\n            loader_train=self.loader_train,\n            loader_test=self.loader_valid,\n            metric_fn=self.metric_fn,\n            optimizer=self.optimizer,\n            lr_scheduler=self.lr_scheduler,\n            device=device,\n            data_getter=itemgetter(\"image\"),\n            target_getter=itemgetter(\"target\"),\n            stage_progress=trainer_config.progress_bar,\n            get_key_metric=itemgetter(\"mAP\"),\n            visualizer=self.visualizer,\n            model_save_best=trainer_config.model_save_best,\n            model_saving_frequency=trainer_config.model_saving_frequency,\n            save_dir=trainer_config.model_dir\n        )\n\n        model_trainer.register_hook(\"train\", hooks.train_hook_faster_rcnn)\n        model_trainer.register_hook(\"test\", hooks.test_hook_faster_rcnn)\n        model_trainer.register_hook(\"end_epoch\", hooks.end_epoch_hook_faster_rcnn)\n        self.metrics = model_trainer.fit(trainer_config.epoch_num)\n        return self.metrics\n\n    def draw_bboxes(self, rows, columns, trainer_config:  trainer_config):\n        # load the best model\n        if trainer_config.model_save_best:\n            self.model.load_state_dict(\n                torch.\n                load(os.path.join(trainer_config.model_dir, self.model.__class__.__name__) + '_best.pth')\n            )\n        # or use the last saved\n        self.model = self.model.eval()\n\n        fig, ax = plt.subplots(\n            nrows=rows, ncols=columns, figsize=(15, 30), gridspec_kw={\n                'wspace': 0,\n                'hspace': 0.05\n            }\n        )\n        \n        colors = [(255, 0, 0), (0, 225, 0), (0, 0, 225)]\n\n        for axi in ax.flat:\n            index = random.randrange(len(self.loader_test.dataset))\n\n            image, targets = self.loader_test.dataset[index]\n\n            device = torch.device(trainer_config.device)\n            image = image.to(device).clone()\n\n            detections = self.model(image.unsqueeze(0))\n            bboxes = detections[0]['boxes'].cpu().detach().numpy()\n            labels = detections[0]['labels'].cpu().detach().numpy()\n            scores = detections[0]['scores'].cpu().detach().numpy()\n\n            with torch.no_grad():\n                img = image.cpu()\n                img = img.numpy().transpose(1, 2, 0)\n                img = (img * 255.).astype(np.uint8)\n                gt_img = img.copy()\n                pred_img = img.copy()\n\n                for i, box in enumerate(targets['boxes']):\n                    label = targets['labels'][i]\n                    cls = self.classes[label]\n                    clr = colors[label-1]\n                    gt_img = cv2.rectangle(\n                        gt_img, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), clr, thickness=2)\n                    gt_img = cv2.putText(gt_img, cls, (int(box[0]), int(box[1])-10), cv2.FONT_HERSHEY_SIMPLEX, \n                                         0.9, clr, 2)\n                    \n                for i, box in enumerate(bboxes):\n                    label = labels[i]\n                    score = scores[i]\n                    cls = self.classes[label]\n                    clr = colors[label-1]\n                    cls_score = '{0}:{1:.2}'.format(cls, score)\n                    pred_img = cv2.rectangle(\n                        pred_img, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), clr, thickness=2)\n                    pred_img = cv2.putText(pred_img, cls_score, (int(box[0]), int(box[1])-10), \n                                           cv2.FONT_HERSHEY_SIMPLEX, 0.9, clr, 2)\n\n                merged_img = np.concatenate((gt_img, pred_img), axis=1)\n                axi.imshow(merged_img)\n                axi.axis('off')\n        fig.show()\n\n    \n    def infer_video(self, video_path: str, output_path: str, trainer_config: trainer_config, conf_threshold: float = 0.5):\n        \"\"\"\n        Perform inference on a video and save the output with detected bounding boxes.\n    \n        Args:\n            video_path (str): Path to the input video.\n            output_path (str): Path to save the output video with detections.\n            trainer_config: Configuration object for the trainer.\n            conf_threshold (float): Confidence threshold to filter predictions.\n        \"\"\"\n        # Load the best model if specified\n        if trainer_config.model_save_best:\n            self.model.load_state_dict(\n                torch.load(os.path.join(trainer_config.model_dir, self.model.__class__.__name__ + '_best.pth'))\n            )\n        self.model = self.model.eval()\n        device = torch.device(trainer_config.device)\n        self.model = self.model.to(device)\n    \n        # Open the video file\n        cap = cv2.VideoCapture(video_path)\n        if not cap.isOpened():\n            raise ValueError(f\"Cannot open video file {video_path}\")\n    \n        # Get video properties\n        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        fps = int(cap.get(cv2.CAP_PROP_FPS))\n        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    \n        # Set up video writer for output\n        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n        out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n    \n        colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255)]  # Colors for different classes\n    \n        print(f\"Processing video: {video_path}\")\n        frame_idx = 0\n        while cap.isOpened():\n            ret, frame = cap.read()\n            if not ret:\n                break\n    \n            # Convert frame to tensor\n            img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            img = Image.fromarray(img)\n            img_tensor = F.to_tensor(img).unsqueeze(0).to(device)\n    \n            # Perform inference\n            with torch.no_grad():\n                detections = self.model(img_tensor)\n            boxes = detections[0]['boxes'].cpu().numpy()\n            labels = detections[0]['labels'].cpu().numpy()\n            scores = detections[0]['scores'].cpu().numpy()\n    \n            # Draw detections on the frame\n            for i, box in enumerate(boxes):\n                if scores[i] < conf_threshold:\n                    continue\n                label = labels[i]\n                score = scores[i]\n                cls = self.classes[label]\n                color = colors[label % len(colors)]\n                frame = cv2.rectangle(frame, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), color, 2)\n                frame = cv2.putText(frame, f\"{cls} {score:.2f}\", (int(box[0]), int(box[1]) - 10),\n                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n    \n            # Write the frame to the output video\n            out.write(frame)\n            frame_idx += 1\n            print(f\"Processed frame {frame_idx}/{frame_count}\", end='\\r')\n    \n        # Release resources\n        cap.release()\n        out.release()\n        print(f\"Video saved to {output_path}\")\n\n    \n    def evaluate_coco(self, trainer_config: trainer_config):\n        \"\"\"\n        Evaluate model performance using COCO Detection metrics.\n    \n        Args:\n            trainer_config: Configuration object for trainer settings.\n        \"\"\"\n        # Prepare the model for inference\n        device = torch.device(trainer_config.device)\n        self.model = self.model.to(device).eval()\n        \n        # COCO-style structures\n        coco_annotations = {\n            \"images\": [],\n            \"annotations\": [],\n            \"categories\": [{\"id\": i, \"name\": name} for i, name in enumerate(self.classes, start=1)]\n        }\n        predictions = []\n        \n        annotation_id = 1\n        \n        print(\"Preparing ground truth and predictions for COCO evaluation...\")\n        \n        for img_id, (img, target) in enumerate(self.loader_valid, start=1):\n            img = img.to(device)\n            target = {k: v.to(device) for k, v in target.items()}\n            \n            # Add an image to COCO annotations\n            coco_annotations[\"images\"].append({\n                \"id\": img_id,\n                \"file_name\": f\"image_{img_id}.jpg\",\n                \"width\": img.shape[-1],\n                \"height\": img.shape[-2]\n            })\n            \n            # Add ground truth bounding boxes to COCO annotations\n            for box, label in zip(target[\"boxes\"], target[\"labels\"]):\n                x_min, y_min, x_max, y_max = box.tolist()\n                coco_annotations[\"annotations\"].append({\n                    \"id\": annotation_id,\n                    \"image_id\": img_id,\n                    \"category_id\": label.item(),\n                    \"bbox\": [x_min, y_min, x_max - x_min, y_max - y_min],\n                    \"area\": (x_max - x_min) * (y_max - y_min),\n                    \"iscrowd\": 0\n                })\n                annotation_id += 1\n            \n            # Model predictions\n            with torch.no_grad():\n                outputs = self.model(img.unsqueeze(0))\n            for box, score, label in zip(\n                    outputs[0]['boxes'].cpu(), \n                    outputs[0]['scores'].cpu(), \n                    outputs[0]['labels'].cpu()):\n                x_min, y_min, x_max, y_max = box.tolist()\n                predictions.append({\n                    \"image_id\": img_id,\n                    \"category_id\": label.item(),\n                    \"bbox\": [x_min, y_min, x_max - x_min, y_max - y_min],\n                    \"score\": score.item()\n                })\n        \n        # Temporary files (in memory)\n        gt_file = os.path.join('/kaggle','working','coco_gt_annotations.json')\n        pred_file = os.path.join('/kaggle','working','coco_predictions.json')\n        \n        with open(gt_file, \"w\") as f:\n            json.dump(coco_annotations, f)\n        \n        with open(pred_file, \"w\") as f:\n            json.dump(predictions, f)\n        \n        # Retrieving annotations using the COCO API\n        coco_gt = COCO(gt_file)\n        coco_dt = coco_gt.loadRes(pred_file)\n        \n        # Evaluation\n        coco_eval = COCOeval(coco_gt, coco_dt, iouType='bbox')\n        coco_eval.evaluate()\n        coco_eval.accumulate()\n        coco_eval.summarize()\n\n    def save_onnx(self, file_name: str, trainer_config: trainer_config):\n        \"\"\"\n        Export the trained model to ONNX format.\n\n        Args:\n            file_name (str): The output file name for the ONNX model.\n            trainer_config: Configuration object for trainer settings.\n        \"\"\"\n        # Load the best model if it exists\n        if trainer_config.model_save_best:\n            model_path = os.path.join(\n                trainer_config.model_dir, f\"{self.model.__class__.__name__}_best.pth\"\n            )\n            if not os.path.exists(model_path):\n                raise FileNotFoundError(f\"Best model not found at {model_path}\")\n            self.model.load_state_dict(torch.load(model_path))\n        \n        # Prepare the model for export\n        device = torch.device(trainer_config.device)\n        self.model = self.model.to(device).eval()\n        \n        # Get a single input sample for export\n        loader_input = torch.utils.data.DataLoader(\n            dataset=self.dataset_valid,\n            batch_size=1,\n            shuffle=False,\n            collate_fn=collate_fn,\n            num_workers=0,\n            pin_memory=True\n        )\n        inputs, _ = next(iter(loader_input))\n        inputs = inputs.to(device)\n        \n        # Define ONNX export parameters\n        dynamic_axes = {\n            \"input\": {0: \"batch_size\"},\n            \"output_boxes\": {0: \"batch_size\"},\n            \"output_scores\": {0: \"batch_size\"},\n            \"output_labels\": {0: \"batch_size\"},\n        }\n        \n        # Export the model\n        print(f\"Exporting model to ONNX format: {file_name}\")\n        torch.onnx.export(\n            self.model,\n            inputs,\n            file_name,\n            export_params=True,\n            opset_version=11,\n            do_constant_folding=True,\n            input_names=[\"input\"],\n            output_names=[\"output_boxes\", \"output_scores\", \"output_labels\"],\n            dynamic_axes=dynamic_axes\n        )\n        print(f\"Model successfully exported to {file_name}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  <font style=\"color:blue\">2.3. Run Experiment</font>","metadata":{}},{"cell_type":"code","source":"if __name__ == '__main__':\n    dataloader_config, trainer_config = patch_configs(epoch_num_to_set=100, batch_size_to_set=2)\n\n    dataset_config = configuration.DatasetConfig(\n        root_dir=\"data\",\n    )\n    \n    optimizer_config = configuration.OptimizerConfig(\n        learning_rate=5e-3, \n        lr_step_milestones=[50], \n        lr_gamma=0.1, \n        momentum=0.9, \n        weight_decay=1e-5\n    )\n    \n    experiment = Experiment(\n        dataset_config=dataset_config, \n        dataloader_config=dataloader_config, \n        optimizer_config=optimizer_config\n    )\n    \n    # Run the experiment / start training\n    experiment.run(trainer_config)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  <font style=\"color:green\">3. Inference [15 Points]</font> \n\n\n\n**You have to make predictions from your trained model on three images from the validation dataset.**\n\n\n\nThe plotted images should be similar to the following:\n\n\n\n<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-p1.png'>\n\n\n\n\n\n\n\n<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-p2.png'>\n\n\n\n\n\n\n\n<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-p3.png'>\n\n\n\n\n","metadata":{}},{"cell_type":"code","source":"if __name__ == '__main__':\n    experiment.draw_bboxes(4, 1, trainer_config)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  <font style=\"color:green\">4. COCO Detection Evaluation [25 Points]</font> \n\n\n\n**You have to evaluate your detection model on COCO detection evaluation metric.**\n\n\n\nFor your reference here is the coco evaluation metric chart:\n\n\n\n\n\n---\n\n\n\n<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/03/c3-w9-coco_metric.png\">\n\n\n\n---\n\n\n\n#### <font style=\"color:red\">The expected `AP` (primary challenge metric) is more than `0.5`.</font>\n\n\n\n**The expected output should look similar to the following:**\n\n\n\n```\n\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.550\n\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.886\n\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.629\n\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.256\n\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.653\n\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.627\n\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.504\n\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.629\n\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.633\n\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.380\n\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.722\n\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.704\n\n```\n\n\n","metadata":{}},{"cell_type":"code","source":"if __name__ == '__main__':\n    experiment.evaluate_coco(trainer_config=trainer_config)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <font style=\"color:green\">5. Run Inference on a Video [15 Points]</font>\n\n\n\n#### [Download the Input Video](https://www.dropbox.com/s/g88o4dx18zpgn8k/projet3-input-video.mp4?dl=1)\n\n\n\n**You have to run inference on a video.** \n\n\n\nYou can download the video from [here](https://www.dropbox.com/s/g88o4dx18zpgn8k/projet3-input-video.mp4?dl=1).\n\n\n\n#### <font style=\"color:red\">Upload the output video on youtube and share the link. Do not upload the video in the lab.</font>","metadata":{}},{"cell_type":"code","source":"from IPython.display import YouTubeVideo, display\n\nvideo = YouTubeVideo(\"18HWHCevFdU\", width=640, height=360)\n\ndisplay(video)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Your output video should have a bounding box around the vehicle registration plate.**","metadata":{}},{"cell_type":"code","source":"video = YouTubeVideo(\"5SgCuee7AMs\", width=640, height=360)\n\ndisplay(video)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == '__main__':\n    video_input_path = os.path.join('/kaggle','input','opencv-evalution-alpr-dataset','cars_ALPR_test','videos','projet3-input-video.mp4')\n    video_output_path = os.path.join('/kaggle', 'working', 'output_projet3_input_video.mp4')\n    \n    experiment.infer_video(video_path=video_input_path, output_path=video_output_path, trainer_config=trainer_config, conf_threshold=0.5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <font style=\"color:green\">6. Save ONNX file</font>","metadata":{}},{"cell_type":"code","source":"if __name__ == '__main__':\n    path_onnx_file = os.path.join('/kaggle', 'working','ALPR_license_plate_car.onnx')\n    experiment.save_onnx(file_name=path_onnx_file, trainer_config=trainer_config)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def zip_folder_with_shutil(source_folder, output_path):\n    '''Function for zip TensorBoard data'''\n    shutil.make_archive(output_path, 'zip', source_folder)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"zip_folder_with_shutil('/kaggle/working/faster_rcnn', '/kaggle/working/faster_rcnn')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"zip_folder_with_shutil('/kaggle/working/modelscheckpoints', '/kaggle/working/modelscheckpoints')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}