{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9756403,"sourceType":"datasetVersion","datasetId":5973896},{"sourceId":10169901,"sourceType":"datasetVersion","datasetId":6280772}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <font style=\"color:blue\">Project 3: Object Detection</font>\n\n\n\n\n\n#### Maximum Points: 100\n\n\n\n|Sr. no.|Section|Points|\n|:-------|:-------|:------:|\n|1|Plot Ground Truth Bounding Boxes|20|\n|2|Training|25|\n|3|Inference|15|\n|4|COCO Detection Evaluation|25|\n|5|COCO Detection Evaluation|15|","metadata":{}},{"cell_type":"markdown","source":"# <font style=\"color:purple\">Download the config file and trainer package</font> ","metadata":{}},{"cell_type":"code","source":"!wget \"https://raw.githubusercontent.com/RadimKozl/OpenCV_PyTorch_Project3/refs/heads/main/trainer.zip\" -O ./trainer.zip\n!ls /kaggle/working/\n!unzip /kaggle/working/trainer.zip\n!rm /kaggle/working/trainer.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T22:01:38.952853Z","iopub.execute_input":"2024-12-11T22:01:38.953243Z","iopub.status.idle":"2024-12-11T22:01:43.841256Z","shell.execute_reply.started":"2024-12-11T22:01:38.953210Z","shell.execute_reply":"2024-12-11T22:01:43.839705Z"}},"outputs":[{"name":"stdout","text":"--2024-12-11 22:01:39--  https://raw.githubusercontent.com/RadimKozl/OpenCV_PyTorch_Project3/refs/heads/main/trainer.zip\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 32212 (31K) [application/zip]\nSaving to: './trainer.zip'\n\n./trainer.zip       100%[===================>]  31.46K  --.-KB/s    in 0.01s   \n\n2024-12-11 22:01:40 (2.32 MB/s) - './trainer.zip' saved [32212/32212]\n\ntrainer.zip\nArchive:  /kaggle/working/trainer.zip\n   creating: trainer/.ipynb_checkpoints/\n  inflating: trainer/.ipynb_checkpoints/configuration-checkpoint.py  \n  inflating: trainer/.ipynb_checkpoints/datasets-checkpoint.py  \n  inflating: trainer/.ipynb_checkpoints/hooks-checkpoint.py  \n  inflating: trainer/.ipynb_checkpoints/matplotlib_visualizer-checkpoint.py  \n  inflating: trainer/.ipynb_checkpoints/trainer-checkpoint.py  \n  inflating: trainer/.ipynb_checkpoints/utils-checkpoint.py  \n  inflating: trainer/.ipynb_checkpoints/voc_eval-checkpoint.py  \n  inflating: trainer/__init__.py     \n  inflating: trainer/base_metric.py  \n  inflating: trainer/configuration.py  \n  inflating: trainer/datasets.py     \n  inflating: trainer/hooks.py        \n  inflating: trainer/matplotlib_visualizer.py  \n  inflating: trainer/metrics.py      \n  inflating: trainer/tensorboard_visualizer.py  \n  inflating: trainer/trainer.py      \n  inflating: trainer/utils.py        \n  inflating: trainer/visualizer.py   \n  inflating: trainer/voc_eval.py     \n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!wget \"https://github.com/RadimKozl/OpenCV_PyTorch_Project3/blob/main/config.yaml\" -O ./config.yaml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T22:01:43.843991Z","iopub.execute_input":"2024-12-11T22:01:43.844473Z","iopub.status.idle":"2024-12-11T22:01:45.460995Z","shell.execute_reply.started":"2024-12-11T22:01:43.844419Z","shell.execute_reply":"2024-12-11T22:01:45.459715Z"}},"outputs":[{"name":"stdout","text":"--2024-12-11 22:01:44--  https://github.com/RadimKozl/OpenCV_PyTorch_Project3/blob/main/config.yaml\nResolving github.com (github.com)... 20.27.177.113\nConnecting to github.com (github.com)|20.27.177.113|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/html]\nSaving to: './config.yaml'\n\n./config.yaml           [ <=>                ] 165.55K  --.-KB/s    in 0.1s    \n\n2024-12-11 22:01:45 (1.59 MB/s) - './config.yaml' saved [169525]\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# <font style=\"color:purple\">Install external libraries</font> ","metadata":{}},{"cell_type":"code","source":"!pip install psutil\n!pip install gpustat\n!pip install orjson","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T22:01:45.462750Z","iopub.execute_input":"2024-12-11T22:01:45.463764Z","iopub.status.idle":"2024-12-11T22:02:15.203189Z","shell.execute_reply.started":"2024-12-11T22:01:45.463698Z","shell.execute_reply":"2024-12-11T22:02:15.201935Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (5.9.3)\nRequirement already satisfied: gpustat in /opt/conda/lib/python3.10/site-packages (1.0.0)\nRequirement already satisfied: six>=1.7 in /opt/conda/lib/python3.10/site-packages (from gpustat) (1.16.0)\nRequirement already satisfied: nvidia-ml-py<=11.495.46,>=11.450.129 in /opt/conda/lib/python3.10/site-packages (from gpustat) (11.495.46)\nRequirement already satisfied: psutil>=5.6.0 in /opt/conda/lib/python3.10/site-packages (from gpustat) (5.9.3)\nRequirement already satisfied: blessed>=1.17.1 in /opt/conda/lib/python3.10/site-packages (from gpustat) (1.20.0)\nRequirement already satisfied: wcwidth>=0.1.4 in /opt/conda/lib/python3.10/site-packages (from blessed>=1.17.1->gpustat) (0.2.13)\nRequirement already satisfied: orjson in /opt/conda/lib/python3.10/site-packages (3.10.4)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T22:02:15.205433Z","iopub.execute_input":"2024-12-11T22:02:15.205783Z","iopub.status.idle":"2024-12-11T22:02:16.234113Z","shell.execute_reply.started":"2024-12-11T22:02:15.205747Z","shell.execute_reply":"2024-12-11T22:02:16.232987Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# <font style=\"color:purple\">Imports libraries</font> ","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport json\nimport orjson\nimport csv\nimport yaml\nimport shutil\nimport pandas as pd\nimport numpy as np\nimport random\nimport time\nimport psutil\nimport gpustat\nimport matplotlib.pyplot as plt\nimport IPython\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom concurrent.futures import ThreadPoolExecutor\n\nfrom operator import itemgetter\nimport multiprocessing as mp\nmp.set_start_method('spawn', force=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T22:02:16.235568Z","iopub.execute_input":"2024-12-11T22:02:16.235932Z","iopub.status.idle":"2024-12-11T22:02:16.692827Z","shell.execute_reply.started":"2024-12-11T22:02:16.235872Z","shell.execute_reply":"2024-12-11T22:02:16.691907Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# <font style=\"color:purple\">Download the Dataset</font> \n\n\n\n**[Download the Vehicle registration plate](https://www.dropbox.com/s/k81ljpmzy3fgtx9/Dataset.zip?dl=1)**\n\n\n\n\n\nDownload the Vehicle Registration Plate dataset from [here](https://www.dropbox.com/s/k81ljpmzy3fgtx9/Dataset.zip?dl=1) and unzip it. \n\n\n\nWe will have the following directory structure:\n\n\n\n```\n\nDataset\n\n├── train\n\n│   └── Vehicle registration plate\n\n│       └── Label\n\n└── validation\n\n    └── Vehicle registration plate\n\n        └── Label\n\n```\n\n\n\nUnzipping the file will give you a directory `Dataset`. This directory has two folder `train` and `validation`. Each train and validation folder has `Vehicle registration plate`  folder with `.jpg` images and a folder `Labels`.  `Labels` folder has bounding box data for the images.\n\n\n\n\n\nFor example,\n\nFor image: `Dataset/train/Vehicle registration plate/bf4689922cdfd532.jpg`\n\nLabel file is  `Dataset/train/Vehicle registration plate/Label/bf4689922cdfd532.txt`\n\n\n\nThere are one or more lines in each `.txt` file. Each line represents one bounding box.\n\nFor example,\n\n```\n\nVehicle registration plate 385.28 445.15 618.24 514.225\n\nVehicle registration plate 839.68 266.066462 874.24 289.091462\n\n```\n\n\n\nWe have a single class detection (`Vehicle registration plate detection`) problem. So bounding box details start from the fourth column in each row.\n\n\n\nRepresentation is in `xmin`, `ymin`, `xmax`, and `ymax` format.\n\n\n\n**It has `5308` training and `386` validation dataset.**\n\n\n\nData is downloaded from [Open Images Dataset](https://storage.googleapis.com/openimages/web/index.html)","metadata":{}},{"cell_type":"markdown","source":"#  <font style=\"color:green\">1. Plot Ground Truth Bounding Boxes [20 Points]</font> \n\n\n\n**You have to show three images from validation data with the bounding boxes.**\n\n\n\nThe plotted images should be similar to the following:\n\n\n\n<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-g1.png'>\n\n\n\n\n\n\n\n<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-g2.png'>\n\n\n\n\n\n\n\n<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-g3.png'>\n\n\n","metadata":{}},{"cell_type":"markdown","source":"##  <font style=\"color:blue\">1.1. Class for create JSON file of datasets</font>\n\nWe create Json structure of `train`, `valid` and `test` datasets, for creating PyTorch Dataloader a standard module of trainer.","metadata":{}},{"cell_type":"code","source":"class TransformDataset:\n    def __init__(self, name_json_file='datasets.json'):\n        self.name_json_file = name_json_file\n        self.path_test_images = os.path.join('/kaggle', 'input', 'opencv-evalution-alpr-dataset', 'cars_ALPR_test')\n        self.path_train_images = os.path.join('/kaggle', 'input', 'vehicle-registration-plate', 'Dataset', 'train', 'Vehicle registration plate')\n        self.path_valid_images = os.path.join('/kaggle', 'input', 'vehicle-registration-plate', 'Dataset', 'validation', 'Vehicle registration plate')\n        self.json_file = {\n            \"datasets\": [\n                {\n                    \"train\": [],\n                    \"valid\": [],\n                    \"test\": [],\n                    \"class_number\": 2,\n                    \"names_class\": ['__background__', 'Vehicle registration plate']\n                }\n            ]\n        }\n\n    def __select_files(self, root_path, endswith_file):\n        return [os.path.join(root_path, filename) for filename in os.listdir(root_path) if filename.endswith(endswith_file)]\n\n    def __get_image_shape(self, image_path):\n        img = Image.open(image_path)\n        w, h = img.size\n        c = img.mode\n        cc = ''\n        if c == 'RGB':\n            cc = 'RGB'\n        elif c == 'RGBA':\n            cc = 'RGBA'\n        elif c == 'L':\n            cc = 'Grayscale'\n        elif c == '1':\n            cc = 'Grayscale'\n        else:\n            cc = c\n        return (w, h, c)\n\n    def __select_data(self, file_path):\n        processed_data = []\n        list_class_name = self.json_file['datasets'][0]['names_class']\n\n        if os.path.isfile(file_path):\n            with open(file_path, 'r') as input_file:\n                lines = input_file.readlines()\n\n            for line in lines:\n                parts = line.strip().split()\n                label = parts[0] + ' ' + parts[1] + ' ' + parts[2]\n                numbers = [round(float(num)) for num in parts[3:]]\n\n                processed_line = {\n                    \"label\": label,\n                    \"idlabel\": int(list_class_name.index(label)),\n                    \"box_coordinates\": {\n                        'xmin': numbers[0],\n                        'ymin': numbers[1],\n                        'xmax': numbers[2],\n                        'ymax': numbers[3]\n                    }\n                }\n                processed_data.append(processed_line)\n        else:\n            print(f\"{str(file_path)} not found!\")\n\n        return processed_data\n\n    def process_files(self, file_paths, dataset_type):\n        data = {}\n        for path in tqdm(file_paths, desc=f\"Processing {dataset_type} files\"):\n            img_dim = self.__get_image_shape(path)\n            name_dir, name_input = os.path.split(path)\n            name_dir = os.path.join(name_dir, 'Label' if dataset_type != 'test' else 'Annotations')\n            id_name, type = name_input.split('.')\n            name_label = id_name + '.txt'\n            path_label = os.path.join(name_dir, name_label)\n            sub_data = self.__select_data(path_label)\n\n            if sub_data:  # Only add to JSON if sub_data is not empty\n                data[str(id_name)] = {\n                    \"name\": str(name_input),\n                    \"path\": str(path),\n                    \"type\": str(type),\n                    \"width\": int(img_dim[0]),\n                    \"height\": int(img_dim[1]),\n                    \"channel\": str(img_dim[2]),\n                    \"boxes\": sub_data\n                }\n                self.json_file['datasets'][0][dataset_type].append(data)\n\n    def create_json_data(self):\n        list_train_files = self.__select_files(self.path_train_images, '.jpg')\n        list_valid_files = self.__select_files(self.path_valid_images, '.jpg')\n        list_test_files = self.__select_files(self.path_test_images, '.jpg')\n\n        # Process train files\n        self.process_files(list_train_files, 'train')\n\n        # Process valid files\n        self.process_files(list_valid_files, 'valid')\n\n        # Process test files\n        self.process_files(list_test_files, 'test')\n\n    def save_json_file(self):\n        with open(os.path.join('/kaggle', 'working', self.name_json_file), 'wb') as f:\n            f.write(orjson.dumps(self.json_file))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T22:02:16.697296Z","iopub.execute_input":"2024-12-11T22:02:16.697596Z","iopub.status.idle":"2024-12-11T22:02:16.716397Z","shell.execute_reply.started":"2024-12-11T22:02:16.697566Z","shell.execute_reply":"2024-12-11T22:02:16.715331Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"##  <font style=\"color:blue\">1.2. Create JSON file of datasets</font>","metadata":{}},{"cell_type":"code","source":"%%time\ntransform_dataset = TransformDataset(name_json_file = 'datasets.json')\ntransform_dataset.create_json_data()\ntransform_dataset.save_json_file()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T22:02:16.717756Z","iopub.execute_input":"2024-12-11T22:02:16.718194Z","iopub.status.idle":"2024-12-11T22:05:05.268083Z","shell.execute_reply.started":"2024-12-11T22:02:16.718136Z","shell.execute_reply":"2024-12-11T22:05:05.267056Z"}},"outputs":[{"name":"stderr","text":"Processing train files: 100%|██████████| 5308/5308 [01:39<00:00, 53.15it/s]\nProcessing valid files: 100%|██████████| 386/386 [00:07<00:00, 51.46it/s]\nProcessing test files: 0it [00:00, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"CPU times: user 27.6 s, sys: 23.8 s, total: 51.5 s\nWall time: 2min 48s\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"#  <font style=\"color:green\">2. Training [25 Points]</font> \n\n\n\n- **Write your training code in this section.**\n\n\n\n- **You also have to share ([shared logs example](https://tensorboard.dev/experiment/JRtnsKbwTaq1ow6nPLPGeg)) the loss plot of your training using tensorboard.dev.** \n\n\n\nHow to share TensorBoard logs using tensorboard.dev find [here](https://courses.opencv.org/courses/course-v1:OpenCV+OpenCV-106+2019_T1/courseware/b1c43ffe765246658e537109e188addb/d62572ec8bd344db9aeae81235ede618/4?activate_block_id=block-v1%3AOpenCV%2BOpenCV-106%2B2019_T1%2Btype%40vertical%2Bblock%40398b46ddcd5c465fa52cb4d572ba3229).","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  <font style=\"color:green\">3. Inference [15 Points]</font> \n\n\n\n**You have to make predictions from your trained model on three images from the validation dataset.**\n\n\n\nThe plotted images should be similar to the following:\n\n\n\n<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-p1.png'>\n\n\n\n\n\n\n\n<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-p2.png'>\n\n\n\n\n\n\n\n<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-p3.png'>\n\n\n\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  <font style=\"color:green\">4. COCO Detection Evaluation [25 Points]</font> \n\n\n\n**You have to evaluate your detection model on COCO detection evaluation metric.**\n\n\n\nFor your reference here is the coco evaluation metric chart:\n\n\n\n\n\n---\n\n\n\n<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/03/c3-w9-coco_metric.png\">\n\n\n\n---\n\n\n\n#### <font style=\"color:red\">The expected `AP` (primary challenge metric) is more than `0.5`.</font>\n\n\n\n**The expected output should look similar to the following:**\n\n\n\n```\n\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.550\n\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.886\n\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.629\n\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.256\n\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.653\n\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.627\n\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.504\n\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.629\n\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.633\n\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.380\n\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.722\n\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.704\n\n```\n\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <font style=\"color:green\">5. Run Inference on a Video [15 Points]</font>\n\n\n\n#### [Download the Input Video](https://www.dropbox.com/s/g88o4dx18zpgn8k/projet3-input-video.mp4?dl=1)\n\n\n\n**You have to run inference on a video.** \n\n\n\nYou can download the video from [here](https://www.dropbox.com/s/g88o4dx18zpgn8k/projet3-input-video.mp4?dl=1).\n\n\n\n#### <font style=\"color:red\">Upload the output video on youtube and share the link. Do not upload the video in the lab.</font>","metadata":{}},{"cell_type":"code","source":"from IPython.display import YouTubeVideo, display\n\nvideo = YouTubeVideo(\"18HWHCevFdU\", width=640, height=360)\n\ndisplay(video)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Your output video should have a bounding box around the vehicle registration plate.**","metadata":{}},{"cell_type":"code","source":"video = YouTubeVideo(\"5SgCuee7AMs\", width=640, height=360)\n\ndisplay(video)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**You can use the following sample code to read and write a video.**","metadata":{}},{"cell_type":"code","source":"def video_read_write(video_path):\n\n    \"\"\"\n\n    Read video frames one-by-one, flip it, and write in the other video.\n\n    video_path (str): path/to/video\n\n    \"\"\"\n\n    video = cv2.VideoCapture(video_path)\n\n    \n\n    # Check if camera opened successfully\n\n    if not video.isOpened(): \n\n        print(\"Error opening video file\")\n\n        return\n\n    \n\n    # create video writer\n\n    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n\n    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    frames_per_second = video.get(cv2.CAP_PROP_FPS)\n\n    num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    \n\n    output_fname = '{}_out.mp4'.format(os.path.splitext(video_path)[0])\n\n    \n\n    output_file = cv2.VideoWriter(\n\n        filename=output_fname,\n\n        # some installation of opencv may not support x264 (due to its license),\n\n        # you can try other format (e.g. MPEG)\n\n        fourcc=cv2.VideoWriter_fourcc(*\"x264\"),\n\n        fps=float(frames_per_second),\n\n        frameSize=(width, height),\n\n        isColor=True,\n\n    )\n\n    \n\n        \n\n    i = 0\n\n    while video.isOpened():\n\n        ret, frame = video.read()\n\n        if ret:\n\n            \n\n            output_file.write(frame[:, ::-1, :])\n\n#             cv2.imwrite('anpd_out/frame_{}.png'.format(str(i).zfill(3)), frame[:, ::-1, :])\n\n            i += 1\n\n        else:\n\n            break\n\n        \n\n    video.release()\n\n    output_file.release()\n\n    \n\n    return","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}