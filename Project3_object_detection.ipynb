{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9756403,"sourceType":"datasetVersion","datasetId":5973896},{"sourceId":10169901,"sourceType":"datasetVersion","datasetId":6280772}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <font style=\"color:blue\">Project 3: Object Detection</font>\n\n\n\n\n\n#### Maximum Points: 100\n\n\n\n|Sr. no.|Section|Points|\n|:-------|:-------|:------:|\n|1|Plot Ground Truth Bounding Boxes|20|\n|2|Training|25|\n|3|Inference|15|\n|4|COCO Detection Evaluation|25|\n|5|COCO Detection Evaluation|15|","metadata":{}},{"cell_type":"markdown","source":"# <font style=\"color:purple\">Download the config file and trainer package</font> ","metadata":{}},{"cell_type":"code","source":"!wget \"https://raw.githubusercontent.com/RadimKozl/OpenCV_PyTorch_Project3/refs/heads/main/trainer.zip\" -O ./trainer.zip\n!ls /kaggle/working/\n!unzip /kaggle/working/trainer.zip\n!rm /kaggle/working/trainer.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:28:07.046904Z","iopub.execute_input":"2024-12-16T09:28:07.047337Z","iopub.status.idle":"2024-12-16T09:28:12.315355Z","shell.execute_reply.started":"2024-12-16T09:28:07.047298Z","shell.execute_reply":"2024-12-16T09:28:12.313552Z"}},"outputs":[{"name":"stdout","text":"--2024-12-16 09:28:08--  https://raw.githubusercontent.com/RadimKozl/OpenCV_PyTorch_Project3/refs/heads/main/trainer.zip\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 33399 (33K) [application/zip]\nSaving to: './trainer.zip'\n\n./trainer.zip       100%[===================>]  32.62K  --.-KB/s    in 0.007s  \n\n2024-12-16 09:28:08 (4.64 MB/s) - './trainer.zip' saved [33399/33399]\n\ntrainer.zip\nArchive:  /kaggle/working/trainer.zip\n   creating: trainer/.ipynb_checkpoints/\n  inflating: trainer/.ipynb_checkpoints/configuration-checkpoint.py  \n  inflating: trainer/.ipynb_checkpoints/datasets-checkpoint.py  \n  inflating: trainer/.ipynb_checkpoints/hooks-checkpoint.py  \n  inflating: trainer/.ipynb_checkpoints/matplotlib_visualizer-checkpoint.py  \n  inflating: trainer/.ipynb_checkpoints/trainer-checkpoint.py  \n  inflating: trainer/.ipynb_checkpoints/utils-checkpoint.py  \n  inflating: trainer/.ipynb_checkpoints/voc_eval-checkpoint.py  \n  inflating: trainer/__init__.py     \n  inflating: trainer/base_metric.py  \n  inflating: trainer/configuration.py  \n  inflating: trainer/datasets.py     \n  inflating: trainer/hooks.py        \n  inflating: trainer/matplotlib_visualizer.py  \n  inflating: trainer/metrics.py      \n  inflating: trainer/tensorboard_visualizer.py  \n  inflating: trainer/trainer.py      \n  inflating: trainer/utils.py        \n  inflating: trainer/visualizer.py   \n  inflating: trainer/voc_eval.py     \n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!wget \"https://github.com/RadimKozl/OpenCV_PyTorch_Project3/blob/main/config.yaml\" -O ./config.yaml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:28:12.318559Z","iopub.execute_input":"2024-12-16T09:28:12.319029Z","iopub.status.idle":"2024-12-16T09:28:13.976661Z","shell.execute_reply.started":"2024-12-16T09:28:12.318979Z","shell.execute_reply":"2024-12-16T09:28:13.975121Z"}},"outputs":[{"name":"stdout","text":"--2024-12-16 09:28:13--  https://github.com/RadimKozl/OpenCV_PyTorch_Project3/blob/main/config.yaml\nResolving github.com (github.com)... 140.82.116.3\nConnecting to github.com (github.com)|140.82.116.3|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/html]\nSaving to: './config.yaml'\n\n./config.yaml           [ <=>                ] 238.50K  --.-KB/s    in 0.03s   \n\n2024-12-16 09:28:13 (8.01 MB/s) - './config.yaml' saved [244223]\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# <font style=\"color:purple\">Install external libraries</font> ","metadata":{}},{"cell_type":"code","source":"!pip install psutil \n!pip install h5py ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:28:13.979050Z","iopub.execute_input":"2024-12-16T09:28:13.979597Z","iopub.status.idle":"2024-12-16T09:28:36.359627Z","shell.execute_reply.started":"2024-12-16T09:28:13.979524Z","shell.execute_reply":"2024-12-16T09:28:36.358129Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (5.9.3)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (3.11.0)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from h5py) (1.26.4)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:28:36.364109Z","iopub.execute_input":"2024-12-16T09:28:36.364667Z","iopub.status.idle":"2024-12-16T09:28:37.525091Z","shell.execute_reply.started":"2024-12-16T09:28:36.364607Z","shell.execute_reply":"2024-12-16T09:28:37.523898Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# <font style=\"color:purple\">Imports libraries</font> ","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport json\nimport h5py\nimport csv\nimport yaml\nimport shutil\nimport pandas as pd\nimport numpy as np\nimport random\nimport time\nimport psutil\nimport matplotlib.patches as patches\nimport matplotlib.pyplot as plt\nimport IPython\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom concurrent.futures import ThreadPoolExecutor\n\nfrom operator import itemgetter\nimport multiprocessing as mp\nmp.set_start_method('spawn', force=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:28:37.526998Z","iopub.execute_input":"2024-12-16T09:28:37.527458Z","iopub.status.idle":"2024-12-16T09:28:38.127349Z","shell.execute_reply.started":"2024-12-16T09:28:37.527408Z","shell.execute_reply":"2024-12-16T09:28:38.126267Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms import functional as F\n\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import MultiStepLR\nfrom torch.nn.parallel import DataParallel\n\ntorch.multiprocessing.set_start_method('spawn', force=True)\n\nimport torchvision.models as models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:28:38.128483Z","iopub.execute_input":"2024-12-16T09:28:38.129128Z","iopub.status.idle":"2024-12-16T09:28:42.816983Z","shell.execute_reply.started":"2024-12-16T09:28:38.129079Z","shell.execute_reply":"2024-12-16T09:28:42.815789Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from trainer.datasets import HDF5Dataset\nfrom trainer.utils import collate_fn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:28:42.818372Z","iopub.execute_input":"2024-12-16T09:28:42.819081Z","iopub.status.idle":"2024-12-16T09:28:57.573027Z","shell.execute_reply.started":"2024-12-16T09:28:42.819043Z","shell.execute_reply":"2024-12-16T09:28:57.571916Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## <font style=\"color:orange\">Function for check memory management</font> ","metadata":{}},{"cell_type":"code","source":"def memory_management():\n    \n    cpu_percent = psutil.cpu_percent(interval=1)\n    print(f\"CPU Usage: {cpu_percent}%\")\n    \n    memory_usage = psutil.virtual_memory()\n    print(f\"Memory Usage: {memory_usage.percent}%\")\n    \n    disk_usage = psutil.disk_usage('/')\n    print(f\"Disk Usage: {disk_usage.percent}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:28:57.574624Z","iopub.execute_input":"2024-12-16T09:28:57.575317Z","iopub.status.idle":"2024-12-16T09:28:57.581717Z","shell.execute_reply.started":"2024-12-16T09:28:57.575281Z","shell.execute_reply":"2024-12-16T09:28:57.580339Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# <font style=\"color:purple\">Download the Dataset</font> \n\n\n\n**[Download the Vehicle registration plate](https://www.dropbox.com/s/k81ljpmzy3fgtx9/Dataset.zip?dl=1)**\n\n\n\n\n\nDownload the Vehicle Registration Plate dataset from [here](https://www.dropbox.com/s/k81ljpmzy3fgtx9/Dataset.zip?dl=1) and unzip it. \n\n\n\nWe will have the following directory structure:\n\n\n\n```\n\nDataset\n\n├── train\n\n│   └── Vehicle registration plate\n\n│       └── Label\n\n└── validation\n\n    └── Vehicle registration plate\n\n        └── Label\n\n```\n\n\n\nUnzipping the file will give you a directory `Dataset`. This directory has two folder `train` and `validation`. Each train and validation folder has `Vehicle registration plate`  folder with `.jpg` images and a folder `Labels`.  `Labels` folder has bounding box data for the images.\n\n\n\n\n\nFor example,\n\nFor image: `Dataset/train/Vehicle registration plate/bf4689922cdfd532.jpg`\n\nLabel file is  `Dataset/train/Vehicle registration plate/Label/bf4689922cdfd532.txt`\n\n\n\nThere are one or more lines in each `.txt` file. Each line represents one bounding box.\n\nFor example,\n\n```\n\nVehicle registration plate 385.28 445.15 618.24 514.225\n\nVehicle registration plate 839.68 266.066462 874.24 289.091462\n\n```\n\n\n\nWe have a single class detection (`Vehicle registration plate detection`) problem. So bounding box details start from the fourth column in each row.\n\n\n\nRepresentation is in `xmin`, `ymin`, `xmax`, and `ymax` format.\n\n\n\n**It has `5308` training and `386` validation dataset.**\n\n\n\nData is downloaded from [Open Images Dataset](https://storage.googleapis.com/openimages/web/index.html)","metadata":{}},{"cell_type":"markdown","source":"#  <font style=\"color:green\">1. Plot Ground Truth Bounding Boxes [20 Points]</font> \n\n\n\n**You have to show three images from validation data with the bounding boxes.**\n\n\n\nThe plotted images should be similar to the following:\n\n\n\n<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-g1.png'>\n\n\n\n\n\n\n\n<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-g2.png'>\n\n\n\n\n\n\n\n<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-g3.png'>\n\n\n","metadata":{}},{"cell_type":"markdown","source":"##  <font style=\"color:blue\">1.1. Class for create HDF5 file of datasets</font>\n\nWe create Json structure of `train`, `valid` and `test` datasets, for creating PyTorch Dataloader a standard module of trainer. We add test samples as [OpenCV_evalution_ALPR_dataset](https://www.kaggle.com/datasets/radimkzl/opencv-evalution-alpr-dataset).","metadata":{}},{"cell_type":"code","source":"class TransformDataset:\n    def __init__(self, name_hdf5_file='datasets.hdf5', hdf5_image=False):\n        self.name_hdf5_file = name_hdf5_file\n        self.path_test_images = os.path.join('/kaggle', 'input', 'opencv-evalution-alpr-dataset', 'cars_ALPR_test', 'images')\n        self.path_test_annotations = os.path.join('/kaggle', 'input', 'opencv-evalution-alpr-dataset', 'cars_ALPR_test','Annotations')\n        self.path_train_images = os.path.join('/kaggle', 'input', 'vehicle-registration-plate', 'Dataset', 'train', 'Vehicle registration plate')\n        self.path_valid_images = os.path.join('/kaggle', 'input', 'vehicle-registration-plate', 'Dataset', 'validation', 'Vehicle registration plate')\n        self.hdf5_file = None\n        self.hdf5_image = hdf5_image\n        \n\n    def __select_files(self, root_path, extensions=('.jpg', '.png')):\n        \"\"\"\n        Select image files from the given directory with specified extensions.\n\n        Args:\n            root_path (str): Path to the directory containing images.\n            extensions (tuple): Valid file extensions.\n\n        Returns:\n            list: List of file paths matching the extensions.\n        \"\"\"\n        if not os.path.exists(root_path):\n            print(f\"Warning: Path {root_path} does not exist!\")\n            return []\n\n        return [os.path.join(root_path, filename) for filename in os.listdir(root_path) if filename.endswith(extensions)]\n\n    def __get_image_shape(self, image_path):\n        \"\"\"\n        Get the shape (width, height, channels) of an image.\n\n        Args:\n            image_path (str): Path to the image file.\n\n        Returns:\n            list: [width, height, channels]\n        \"\"\"\n        channel = None\n        img = Image.open(image_path)\n        w, h = img.size\n        c = img.mode\n        if c == 'RGB':\n            channel = 3\n        return [w, h, channel]\n\n    def __select_data(self, file_path):\n        \"\"\"\n        Parse annotation data from a file.\n\n        Args:\n            file_path (str): Path to the annotation file.\n\n        Returns:\n            tuple: Tuple of data (label_id, box_coordinates)\n        \"\"\"\n        box_coordinates = []\n        labels_id = []\n        list_class_name = ['__background__', 'Vehicle registration plate']\n\n        if not os.path.isfile(file_path):\n            print(f\"Warning: Annotation file {file_path} not found!\")\n            return []\n\n        with open(file_path, 'r') as input_file:\n            lines = input_file.readlines()\n\n        for line in lines:\n            parts = line.strip().split()\n            if len(parts) < 5:\n                print(f\"Warning: Invalid annotation format in file {file_path}: {line.strip()}\")\n                continue\n\n            label = parts[0] + ' ' + parts[1] + ' ' + parts[2]\n            numbers = [round(float(num)) for num in parts[3:]]\n\n            labels_id.append(list_class_name.index(label))\n            box_coordinates.append(numbers)\n\n        return (labels_id, box_coordinates)\n\n    def process_files(self, file_paths, dataset_type):\n        \"\"\"\n        Process image and annotation files and add them to the HDF5 dataset.\n\n        Args:\n            file_paths (list): List of image file paths.\n            dataset_type (str): Dataset type ('train', 'valid', 'test').\n        \"\"\"\n        for path in tqdm(file_paths, desc=f\"Processing {dataset_type} files\"):\n            try:\n                # Get image dimensions\n                img_dim = self.__get_image_shape(path)\n                name_dir, name_input = os.path.split(path)\n                id_name, file_ext = os.path.splitext(name_input)\n\n                # Determine label file path\n                if dataset_type == 'test':\n                    # Test dataset annotations are in a separate directory\n                    path_label = os.path.join(self.path_test_annotations, f\"{id_name}.txt\")\n                else:\n                    annotations_dir = os.path.join(name_dir, 'Label')\n                    path_label = os.path.join(annotations_dir, f\"{id_name}.txt\")\n\n                # Read and process annotation data\n                sub_data = self.__select_data(path_label)\n                if not sub_data:\n                    print(f\"Warning: No annotations found for {path_label}. Skipping.\")\n                    continue\n\n                # Prepare data dictionary\n                data = {\n                    \"name_id\": str(id_name),\n                    \"name\": str(name_input),\n                    \"path\": str(path),\n                    \"type\": file_ext.lstrip('.'),\n                    \"dimension\": img_dim,\n                    \"labels\": sub_data[0],\n                    \"boxes\": sub_data[1]\n                }\n\n                try:\n                    # Serialize data and write to HDF5\n                    # set dataset group\n                    dataset_group = self.hdf5_file[dataset_type]\n                    # Create a subgroup for the image\n                    image_group = dataset_group.create_group(data['name_id'])\n                    if self.hdf5_image:\n                        # Add image path, labels, and boxes to the image group\n                        img = Image.open(path)\n                        img_array = np.array(img)\n                        image_group.create_dataset('image', data=img_array)\n                    \n                        image_group.attrs['name'] = np.array(data['name'], dtype='S100')\n                        image_group.attrs['type'] = np.array(data['type'], dtype='S3')\n                        image_group.attrs['dimension'] = np.array(data['dimension'], dtype='float64')\n                        image_group.attrs['labels'] = data['labels']\n                        image_group.attrs['boxes'] = data['boxes']\n                        image_group.attrs['link'] = np.array(path, dtype='S200')\n                    else:\n                        image_group.create_dataset('image_link', data=[path])\n                        image_group.attrs['name'] = np.array(data['name'], dtype='S100')\n                        image_group.attrs['type'] = np.array(data['type'], dtype='S3')\n                        image_group.attrs['dimension'] = np.array(data['dimension'], dtype='float64')\n                        image_group.attrs['labels'] = data['labels']\n                        image_group.attrs['boxes'] = data['boxes']\n                        \n                except Exception as e:\n                    print(f\"Error processing: {e}\")\n            \n\n            except Exception as e:\n                print(f\"Error processing file {path}: {e}\")\n\n    def create_hdf5_data(self):\n        \"\"\"\n        Create HDF5 dataset file with train, valid, and test groups.\n        \"\"\"\n        # Select files for each dataset type\n        list_train_files = self.__select_files(self.path_train_images, ('.jpg', '.png'))\n        list_valid_files = self.__select_files(self.path_valid_images, ('.jpg', '.png'))\n        list_test_files = self.__select_files(self.path_test_images, ('.jpg', '.png'))\n\n        # Validate file lists\n        if not list_train_files:\n            print(\"No training files found!\")\n        if not list_valid_files:\n            print(\"No validation files found!\")\n        if not list_test_files:\n            print(\"No test files found!\")\n\n        with h5py.File(os.path.join('/kaggle', 'working', self.name_hdf5_file), 'w') as self.hdf5_file:\n            # Create dataset groups\n            self.hdf5_file.create_group('train')\n            self.hdf5_file.create_group('valid')\n            self.hdf5_file.create_group('test')\n\n            # Add metadata attributes\n            self.hdf5_file.attrs['class_number'] = 2\n            self.hdf5_file.attrs['names_class'] = ['__background__', 'Vehicle registration plate']\n\n            # Process and add files\n            self.process_files(list_train_files, 'train')\n            self.process_files(list_valid_files, 'valid')\n            self.process_files(list_test_files, 'test')\n\n    def close(self):\n        if hasattr(self, 'name_hdf5_file') and self.name_hdf5_file:\n            self.hdf5_file.close()\n        else:\n            try:\n                f = h5py.File(os.path.join('/kaggle', 'working', self.name_hdf5_file), 'w')\n                f.close()\n                print(\"File is closed...\")\n            except:\n                  print(\"File is still locked, we close file...\")  \n            finally:\n                self.hdf5_file.close()\n\n        print(\"HDF5 dataset created successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:28:57.584073Z","iopub.execute_input":"2024-12-16T09:28:57.584539Z","iopub.status.idle":"2024-12-16T09:28:57.629070Z","shell.execute_reply.started":"2024-12-16T09:28:57.584490Z","shell.execute_reply":"2024-12-16T09:28:57.627873Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"##  <font style=\"color:blue\">1.2. Create HDF5 file of datasets</font>","metadata":{}},{"cell_type":"code","source":"%%time\ntransform_dataset = TransformDataset(name_hdf5_file='datasets.hdf5', hdf5_image=False)\ntransform_dataset.create_hdf5_data()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:28:57.634025Z","iopub.execute_input":"2024-12-16T09:28:57.634524Z","iopub.status.idle":"2024-12-16T09:30:05.325525Z","shell.execute_reply.started":"2024-12-16T09:28:57.634486Z","shell.execute_reply":"2024-12-16T09:30:05.324119Z"}},"outputs":[{"name":"stderr","text":"Processing train files: 100%|██████████| 5308/5308 [01:02<00:00, 84.93it/s]\nProcessing valid files: 100%|██████████| 386/386 [00:04<00:00, 85.38it/s]\nProcessing test files: 100%|██████████| 30/30 [00:00<00:00, 59.34it/s]","output_type":"stream"},{"name":"stdout","text":"CPU times: user 12.2 s, sys: 4.69 s, total: 16.9 s\nWall time: 1min 7s\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"transform_dataset.close()\ndel transform_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:30:05.328397Z","iopub.execute_input":"2024-12-16T09:30:05.328845Z","iopub.status.idle":"2024-12-16T09:30:05.334926Z","shell.execute_reply.started":"2024-12-16T09:30:05.328807Z","shell.execute_reply":"2024-12-16T09:30:05.333446Z"}},"outputs":[{"name":"stdout","text":"HDF5 dataset created successfully!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"gc.collect()\nmemory_management()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:30:05.336368Z","iopub.execute_input":"2024-12-16T09:30:05.336761Z","iopub.status.idle":"2024-12-16T09:30:06.723424Z","shell.execute_reply.started":"2024-12-16T09:30:05.336724Z","shell.execute_reply":"2024-12-16T09:30:06.721921Z"}},"outputs":[{"name":"stdout","text":"CPU Usage: 1.0%\nMemory Usage: 5.0%\nDisk Usage: 74.6%\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"##  <font style=\"color:blue\">1.3. Create Dataloaders for read data</font>","metadata":{}},{"cell_type":"markdown","source":"###  <font style=\"color:orange\">Plot function of samples of data</font>","metadata":{}},{"cell_type":"code","source":"def plot_images(loader, rows=2, columns=3):\n    \"\"\"\n    Plot few images with their bounding boxes.\n\n    Args:\n        loader (DataLoader): DataLoader containing the dataset.\n        rows (int): Number of rows in the plot.\n        columns (int): Number of columns in the plot.\n    \"\"\"\n    plt.rcParams[\"figure.figsize\"] = (15, 9)\n    plt.figure()\n\n    for images, targets in loader:\n        for i in range(min(len(targets['labels']), rows * columns)):\n            plt.subplot(rows, columns, i + 1)\n            img = F.to_pil_image(images[i])\n            plt.imshow(img)\n\n            # Get bounding boxes and labels\n            boxes = targets['boxes'][i].numpy()\n            labels = targets['labels'][i].numpy()\n\n            # Create an Axes instance\n            ax = plt.gca()\n\n            # Plot each bounding box\n            for box, label in zip(boxes, labels):\n                xmin, ymin, xmax, ymax = box\n                width = xmax - xmin\n                height = ymax - ymin\n                rect = patches.Rectangle((xmin, ymin), width, height, linewidth=2, edgecolor='r', facecolor='none')\n                ax.add_patch(rect)\n                plt.text(xmin, ymin, f'Label: {label}', color='red', fontsize=12, backgroundcolor='white')\n\n            plt.axis('off')\n\n        plt.show()\n        break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:30:06.724906Z","iopub.execute_input":"2024-12-16T09:30:06.725266Z","iopub.status.idle":"2024-12-16T09:30:06.735266Z","shell.execute_reply.started":"2024-12-16T09:30:06.725233Z","shell.execute_reply":"2024-12-16T09:30:06.733992Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"###  <font style=\"color:orange\">1.3.1 Create train dataset</font>","metadata":{}},{"cell_type":"code","source":"path_hdf5_file = os.path.join('/kaggle','working','datasets.hdf5')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:30:06.737018Z","iopub.execute_input":"2024-12-16T09:30:06.737509Z","iopub.status.idle":"2024-12-16T09:30:06.749371Z","shell.execute_reply.started":"2024-12-16T09:30:06.737456Z","shell.execute_reply":"2024-12-16T09:30:06.748308Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"%%time\nhdf5_train_dataset = HDF5Dataset(path_hdf5_file, 'train')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:30:06.750819Z","iopub.execute_input":"2024-12-16T09:30:06.751172Z","iopub.status.idle":"2024-12-16T09:30:06.777020Z","shell.execute_reply.started":"2024-12-16T09:30:06.751139Z","shell.execute_reply":"2024-12-16T09:30:06.775673Z"}},"outputs":[{"name":"stdout","text":"CPU times: user 8.79 ms, sys: 1.96 ms, total: 10.7 ms\nWall time: 10.6 ms\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"%%time\n# Calculate mean and std of train dataset\nmean_train, std_train = hdf5_train_dataset.calculate_mean_std_manual()\nprint(f\"Calculated Mean: {mean_train}, Std: {std_train} of train dataset.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:30:06.778487Z","iopub.execute_input":"2024-12-16T09:30:06.778884Z","iopub.status.idle":"2024-12-16T09:33:58.967673Z","shell.execute_reply.started":"2024-12-16T09:30:06.778848Z","shell.execute_reply":"2024-12-16T09:33:58.966265Z"}},"outputs":[{"name":"stdout","text":"Calculated Mean: [0.4346074  0.43316259 0.42894789], Std: [0.28065935 0.28091289 0.29091193] of train dataset.\nCPU times: user 3min 41s, sys: 4.64 s, total: 3min 46s\nWall time: 3min 52s\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# show samples\ntrain_dataloader = torch.utils.data.DataLoader(hdf5_train_dataset, batch_size=6, shuffle=True)\n#plot_images(train_dataloader, rows=2, columns=3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:33:58.969464Z","iopub.execute_input":"2024-12-16T09:33:58.969915Z","iopub.status.idle":"2024-12-16T09:33:58.975884Z","shell.execute_reply.started":"2024-12-16T09:33:58.969876Z","shell.execute_reply":"2024-12-16T09:33:58.974529Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"###  <font style=\"color:orange\">1.3.2 Create valid dataset</font>","metadata":{}},{"cell_type":"code","source":"%%time\nhdf5_valid_dataset = HDF5Dataset(path_hdf5_file, 'valid')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:33:58.977295Z","iopub.execute_input":"2024-12-16T09:33:58.977653Z","iopub.status.idle":"2024-12-16T09:33:59.001018Z","shell.execute_reply.started":"2024-12-16T09:33:58.977615Z","shell.execute_reply":"2024-12-16T09:33:58.999190Z"}},"outputs":[{"name":"stdout","text":"CPU times: user 2.45 ms, sys: 21 µs, total: 2.47 ms\nWall time: 2.13 ms\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"%%time\n# Calculate mean and std of train dataset\nmean_valid, std_valid = hdf5_valid_dataset.calculate_mean_std_manual()\nprint(f\"Calculated Mean: {mean_valid}, Std: {std_valid} of valid dataset.\")\ndel mean_valid, std_valid","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:33:59.003130Z","iopub.execute_input":"2024-12-16T09:33:59.003674Z","iopub.status.idle":"2024-12-16T09:34:16.685183Z","shell.execute_reply.started":"2024-12-16T09:33:59.003591Z","shell.execute_reply":"2024-12-16T09:34:16.683992Z"}},"outputs":[{"name":"stdout","text":"Calculated Mean: [0.40982729 0.40347295 0.39792533], Std: [0.2813981  0.27871906 0.28322028] of valid dataset.\nCPU times: user 16.9 s, sys: 386 ms, total: 17.2 s\nWall time: 17.7 s\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# show samples\nvalid_dataloader = torch.utils.data.DataLoader(hdf5_valid_dataset, batch_size=6, shuffle=True)\n#plot_images(valid_dataloader, rows=2, columns=3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:34:16.686491Z","iopub.execute_input":"2024-12-16T09:34:16.686846Z","iopub.status.idle":"2024-12-16T09:34:16.692275Z","shell.execute_reply.started":"2024-12-16T09:34:16.686813Z","shell.execute_reply":"2024-12-16T09:34:16.691153Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"###  <font style=\"color:orange\">1.3.3 Create test dataset</font>","metadata":{}},{"cell_type":"code","source":"%%time\nhdf5_test_dataset = HDF5Dataset(path_hdf5_file, 'test')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:34:16.693935Z","iopub.execute_input":"2024-12-16T09:34:16.694342Z","iopub.status.idle":"2024-12-16T09:34:16.709137Z","shell.execute_reply.started":"2024-12-16T09:34:16.694297Z","shell.execute_reply":"2024-12-16T09:34:16.708022Z"}},"outputs":[{"name":"stdout","text":"CPU times: user 1.46 ms, sys: 22 µs, total: 1.48 ms\nWall time: 1.24 ms\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"%%time\n# Calculate mean and std of train dataset\nmean_test, std_test = hdf5_test_dataset.calculate_mean_std_manual()\nprint(f\"Calculated Mean: {mean_test}, Std: {std_test} of train dataset.\")\ndel mean_test, std_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:34:16.710727Z","iopub.execute_input":"2024-12-16T09:34:16.711051Z","iopub.status.idle":"2024-12-16T09:34:46.924560Z","shell.execute_reply.started":"2024-12-16T09:34:16.711020Z","shell.execute_reply":"2024-12-16T09:34:46.923448Z"}},"outputs":[{"name":"stdout","text":"Calculated Mean: [0.45828777 0.44046033 0.39260295], Std: [0.30443188 0.29303649 0.30604451] of train dataset.\nCPU times: user 25.8 s, sys: 4.27 s, total: 30.1 s\nWall time: 30.2 s\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# show samples\ntest_dataloader = torch.utils.data.DataLoader(hdf5_test_dataset, batch_size=6, shuffle=True)\n#plot_images(test_dataloader, rows=2, columns=3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:34:46.925955Z","iopub.execute_input":"2024-12-16T09:34:46.926287Z","iopub.status.idle":"2024-12-16T09:34:46.931687Z","shell.execute_reply.started":"2024-12-16T09:34:46.926256Z","shell.execute_reply":"2024-12-16T09:34:46.930550Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"###  <font style=\"color:orange\">1.3.4 Show class names</font>","metadata":{}},{"cell_type":"code","source":"list_class_names = hdf5_train_dataset.names_class\n\n# Access metadata\nprint(f\"Class Number: {hdf5_train_dataset.class_number}\")\nprint(f\"Class Names: {list_class_names}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:34:46.932702Z","iopub.execute_input":"2024-12-16T09:34:46.932986Z","iopub.status.idle":"2024-12-16T09:34:46.944714Z","shell.execute_reply.started":"2024-12-16T09:34:46.932958Z","shell.execute_reply":"2024-12-16T09:34:46.943774Z"}},"outputs":[{"name":"stdout","text":"Class Number: 2\nClass Names: ['__background__', 'Vehicle registration plate']\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"#  <font style=\"color:green\">2. Training [25 Points]</font> \n\n\n\n- **Write your training code in this section.**\n\n\n\n- **You also have to share ([shared logs example](https://tensorboard.dev/experiment/JRtnsKbwTaq1ow6nPLPGeg)) the loss plot of your training using tensorboard.dev.** \n\n\n\nHow to share TensorBoard logs using tensorboard.dev find [here](https://courses.opencv.org/courses/course-v1:OpenCV+OpenCV-106+2019_T1/courseware/b1c43ffe765246658e537109e188addb/d62572ec8bd344db9aeae81235ede618/4?activate_block_id=block-v1%3AOpenCV%2BOpenCV-106%2B2019_T1%2Btype%40vertical%2Bblock%40398b46ddcd5c465fa52cb4d572ba3229).","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  <font style=\"color:green\">3. Inference [15 Points]</font> \n\n\n\n**You have to make predictions from your trained model on three images from the validation dataset.**\n\n\n\nThe plotted images should be similar to the following:\n\n\n\n<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-p1.png'>\n\n\n\n\n\n\n\n<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-p2.png'>\n\n\n\n\n\n\n\n<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-p3.png'>\n\n\n\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  <font style=\"color:green\">4. COCO Detection Evaluation [25 Points]</font> \n\n\n\n**You have to evaluate your detection model on COCO detection evaluation metric.**\n\n\n\nFor your reference here is the coco evaluation metric chart:\n\n\n\n\n\n---\n\n\n\n<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/03/c3-w9-coco_metric.png\">\n\n\n\n---\n\n\n\n#### <font style=\"color:red\">The expected `AP` (primary challenge metric) is more than `0.5`.</font>\n\n\n\n**The expected output should look similar to the following:**\n\n\n\n```\n\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.550\n\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.886\n\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.629\n\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.256\n\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.653\n\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.627\n\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.504\n\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.629\n\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.633\n\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.380\n\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.722\n\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.704\n\n```\n\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <font style=\"color:green\">5. Run Inference on a Video [15 Points]</font>\n\n\n\n#### [Download the Input Video](https://www.dropbox.com/s/g88o4dx18zpgn8k/projet3-input-video.mp4?dl=1)\n\n\n\n**You have to run inference on a video.** \n\n\n\nYou can download the video from [here](https://www.dropbox.com/s/g88o4dx18zpgn8k/projet3-input-video.mp4?dl=1).\n\n\n\n#### <font style=\"color:red\">Upload the output video on youtube and share the link. Do not upload the video in the lab.</font>","metadata":{}},{"cell_type":"code","source":"from IPython.display import YouTubeVideo, display\n\nvideo = YouTubeVideo(\"18HWHCevFdU\", width=640, height=360)\n\ndisplay(video)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Your output video should have a bounding box around the vehicle registration plate.**","metadata":{}},{"cell_type":"code","source":"video = YouTubeVideo(\"5SgCuee7AMs\", width=640, height=360)\n\ndisplay(video)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**You can use the following sample code to read and write a video.**","metadata":{}},{"cell_type":"code","source":"def video_read_write(video_path):\n\n    \"\"\"\n\n    Read video frames one-by-one, flip it, and write in the other video.\n\n    video_path (str): path/to/video\n\n    \"\"\"\n\n    video = cv2.VideoCapture(video_path)\n\n    \n\n    # Check if camera opened successfully\n\n    if not video.isOpened(): \n\n        print(\"Error opening video file\")\n\n        return\n\n    \n\n    # create video writer\n\n    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n\n    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    frames_per_second = video.get(cv2.CAP_PROP_FPS)\n\n    num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    \n\n    output_fname = '{}_out.mp4'.format(os.path.splitext(video_path)[0])\n\n    \n\n    output_file = cv2.VideoWriter(\n\n        filename=output_fname,\n\n        # some installation of opencv may not support x264 (due to its license),\n\n        # you can try other format (e.g. MPEG)\n\n        fourcc=cv2.VideoWriter_fourcc(*\"x264\"),\n\n        fps=float(frames_per_second),\n\n        frameSize=(width, height),\n\n        isColor=True,\n\n    )\n\n    \n\n        \n\n    i = 0\n\n    while video.isOpened():\n\n        ret, frame = video.read()\n\n        if ret:\n\n            \n\n            output_file.write(frame[:, ::-1, :])\n\n#             cv2.imwrite('anpd_out/frame_{}.png'.format(str(i).zfill(3)), frame[:, ::-1, :])\n\n            i += 1\n\n        else:\n\n            break\n\n        \n\n    video.release()\n\n    output_file.release()\n\n    \n\n    return","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}